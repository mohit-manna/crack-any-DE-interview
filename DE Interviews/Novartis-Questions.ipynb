{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b672c714",
   "metadata": {},
   "source": [
    "# Novartis Interview Questions\n",
    "1. **How do you debug your spark code**\n",
    "Mentioned about data skewness, data spillage and how to resolve them\n",
    "\n",
    "    Follow up questions:\n",
    "\n",
    "    i. **Repartition vs coalesce**\n",
    "\n",
    "    ii. **Salting**\n",
    "\n",
    "    iii. **Broadcast join**\n",
    "\n",
    "    iv. **Caching**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c6f111",
   "metadata": {},
   "source": [
    "\n",
    "2. **What can we infer from spark UI and how spark prepares it execution planning??**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdc3519",
   "metadata": {},
   "source": [
    "3. **Why do we use window functions and name few of them??**\n",
    "\n",
    "\n",
    "These function are used to perform calculations across a set of rows( a window) related to the current row.\n",
    "\n",
    "| Function | Description |\n",
    "| :-- | :-- |\n",
    "| ROW_NUMBER() | Assigns a unique sequential number to each row within a partition. |\n",
    "| RANK() | Assigns a rank to each row within a partition, with gaps for duplicate ranks. |\n",
    "| DENSE_RANK() | Similar to RANK(), but without gaps in rank values for duplicate ranks. |\n",
    "| NTILE(n) | Divides rows into n buckets and assigns a bucket number to each row. |\n",
    "| SUM(column) | Computes the sum of values over the window. |\n",
    "| AVG(column) | Computes the average of values over the window. |\n",
    "| MIN(column) | Finds the minimum value over the window. |\n",
    "| MAX(column) | Finds the maximum value over the window. |\n",
    "| COUNT(column) | Counts the number of rows in the window. |\n",
    "| FIRST_VALUE(column) | Returns the first value in the window. |\n",
    "| LAST_VALUE(column) | Returns the last value in the window. |\n",
    "| LEAD(column, n) | Returns the value of the column n rows ahead of the current row. |\n",
    "| LAG(column, n) | Returns the value of the column n rows behind the current row. |\n",
    "| CUME_DIST() | Computes the cumulative distribution of a value within a partition. |\n",
    "| PERCENT_RANK() | Computes the relative rank of a row as a percentage of the total rows. |\n",
    "| NTH_VALUE(column, n) | Returns the nth value in the window. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2513d5e",
   "metadata": {},
   "source": [
    "\n",
    "4. **Default join spark uses**\n",
    "    \n",
    "| Join Strategy | When to Use | Advantages | Disadvantages |\n",
    "| :-- | :-- | :-- | :-- |\n",
    "| Broadcast Hash Join (BHJ) | One dataset is small enough to fit in memory. | Fast, avoids shuffling. | Limited by memory size. |\n",
    "| Shuffle Hash Join (SHJ) | Both datasets are medium-sized and hashable. | Efficient for medium datasets. | Requires shuffling. |\n",
    "| Sort-Merge Join (SMJ) | Both datasets are large and sortable. | Efficient for large datasets. | Sorting and shuffling are expensive. |\n",
    "| Cartesian Join | No join condition (Cartesian product). | Useful for generating combinations. | Extremely expensive for large datasets. |\n",
    "| Broadcast Nested Loop Join | Non-equijoin (e.g., > or <) with one small dataset. | Works for non-equijoin conditions. | Inefficient for large datasets. |\n",
    "| Shuffle-and-Replicate NLJ | Non-equijoin with both large datasets. | Works for non-equijoin conditions. | Very expensive. |\n",
    "| Skew Join | Data is highly skewed. | Handles skewed data efficiently. | Requires additional configuration. |\n",
    "\n",
    "\n",
    "**Follow up questions:**\n",
    "\n",
    "i. **how shuffling happens in sort-merge join, its advantages and disadvantages**\n",
    "\n",
    "Steps:\n",
    "- Partitioning: Data is partitioned based on the join keys. Spark uses a hash function to distribute rows with the same join key to the same partition across the cluster.\n",
    "- Shuffling: Data is shuffled across the network to ensure that rows with the same join key co-located in the same partition. Consumes network I/O and time. \n",
    "- Sorting: Each partition is sorted by the join keys. Sorting ensures that rows with the same join key are grouped together, making the merge step efficient\n",
    "- Merging: Sorted partitions are merged to produce the final result. Rows from both datasets are iterated over and matched based on the join key.\n",
    "\n",
    "Advantages:\n",
    "- Efficient for large datasets: SMJ is suitable for large datasets that can be sorted and merged efficiently.\n",
    "- Optimized for euijoin conditions: It works well for equijoin conditions, where the join keys are equal.\n",
    "- Handles Skewed Data Better: better than hash-based joins because sorting ensures that rows with the same key are grouped together.\n",
    "- Default: when equi-join conditions are present.\n",
    "\n",
    "Disadvantages:\n",
    "- Expensive: Sorting and shuffling can be expensive in terms of time and resources, especially for large datasets.\n",
    "- Memory Usage: Requires sufficient memory to hold the sorted partitions, which can lead to out-of-memory errors if the data is too large.\n",
    "- Not Suitable for Non-Equijoin Conditions: SMJ is not suitable for non-equijoin conditions (e.g., >, <) as it relies on sorting based on equality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afaa3b7",
   "metadata": {},
   "source": [
    "\n",
    "5. **Scenarios when Driver gets OOM issues**\n",
    "\n",
    "Mentioned about collect() and show() - They also gave hint about broadcast datasets also roots to OOM in driver and then I explained it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8a960c",
   "metadata": {},
   "source": [
    "6. **Advantages of Parquet files compared to other file formats**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30d751",
   "metadata": {},
   "source": [
    "7. **How do you decide on number of executors when procssing huge volumes of data given huge cluster size** \n",
    "- Expalined about fat executor, thin executor concepts and given advantages\n",
    "  and disadvantages of both of them and eventually said it depends on time/memory/resource constraints of the usecase\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a262132",
   "metadata": {},
   "source": [
    "8. **What is small file issue in spark and how do we resolve them**\n",
    " - Mentioned about having less number of partition cols and combining multiple small files to a large file, optimize  \n",
    "  in databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f011e6a",
   "metadata": {},
   "source": [
    "9. Get all categories with average price more than 1000$\n",
    "\n",
    "product_table:\n",
    "```csv\n",
    "ID,name,category,price\n",
    "1,Product A,Category 1,1500\n",
    "2,Product B,Category 2,800\n",
    "3,Product C,Category 1,1200\n",
    "4,Product D,Category 3,2000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a26d4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 12:11:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  category|\n",
      "+----------+\n",
      "|Category 1|\n",
      "|Category 3|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Manna\").getOrCreate()\n",
    "columns = [\"ID\",\"name\",\"category\",\"price\"]\n",
    "data = [\n",
    "    (1, \"Product A\", \"Category 1\", 1500),\n",
    "    (2, \"Product B\", \"Category 2\", 800),\n",
    "    (3, \"Product C\", \"Category 1\", 1200),\n",
    "    (4, \"Product D\", \"Category 3\", 2000)\n",
    "]\n",
    "spark.sparkContext.setJobDescription(\"Loading data from CSV file\")\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.createOrReplaceTempView(\"products\")\n",
    "spark.sparkContext.setJobDescription(\"Applying Transformation\")\n",
    "res=spark.sql(\"\"\"\n",
    "    select category\n",
    "    from products group by category\n",
    "    having avg(price) > 1000\n",
    "\"\"\")\n",
    "spark.sparkContext.setJobDescription(\"Showing Data\")\n",
    "res.show()\n",
    "input(\"Press Enter to stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142eb62a",
   "metadata": {},
   "source": [
    "10. **How can this below code be optimized?**\n",
    "\n",
    "```python\n",
    "user = (\n",
    "\tuser.select(\"id\", \"name\")\n",
    "\t.join(address.select(\"id\", \"country\"), \"id\", \"left\")\n",
    "\t.filter(col(\"country\") != \"USA\")\n",
    ")\n",
    "```\n",
    "\n",
    "May be we can use `broadcast` join if address table is small enough to fit in memory.\n",
    "Also, we can filter USAs before the join to reduce the data size being processed.\n",
    "\n",
    "```python\n",
    "user = (\n",
    "\tuser.select(\"id\", \"name\")\n",
    "\t.join(broadcast(address.select(\"id\", \"country\").filter(col(\"country\") != \"USA\")), \"id\", \"left\")\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f300f9",
   "metadata": {},
   "source": [
    "11. How many stages, jobs created as part of below code\n",
    "\n",
    "```python\n",
    "df = (\n",
    "\tdf.select(\"appliance\", \"order_num\")\n",
    "\t.repartition(5)\n",
    "\t.filter(col(\"order_num\").isNotNull())\n",
    "\t.join(order_df.select(\"appliance\", \"category\", \"sub_category\"), \"appliance\")\n",
    "\t.filter(col(\"sub_category\") != \"phone\")\n",
    "\t.groupBy(\"category\")\n",
    "\t.count(agg(collect_set(\"order_num\")).alias(\"order_list\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737edc3",
   "metadata": {},
   "source": [
    "4. Question: \n",
    "Given the DOSE and TESTS tables, for each measurement in the TESTS table, assign the most recent dose (from the DOSE table) for the same subject_id where the dose time is less than or equal to the measurement time. If there is no such dose, leave the dose column blank.\n",
    "\n",
    "**DOSES**\n",
    "| subject_id |    time     | dose |\n",
    "|:----------:|:----------:|:----:|\n",
    "|    01      | 1751878800 |  5   |\n",
    "|    01      | 1752052500 | 10   |\n",
    "|    01      | 1752224160 | 15   |\n",
    "|    02      | 1751878920 | 15   |\n",
    "|    02      | 1752051900 | 12   |\n",
    "|    02      | 1752224340 | 10   |\n",
    "\n",
    "**TESTS**\n",
    "\n",
    "| subject_id |    time     | mesurement |\n",
    "|:----------:|:----------:|:----------:|\n",
    "|    01      | 1751875000 |    2.5     |\n",
    "|    01      | 1751954400 |    2.9     |\n",
    "|    01      | 1752226200 |    2.7     |\n",
    "|    02      | 1751880000 |    2.0     |\n",
    "|    02      | 1752092600 |    2.0     |\n",
    "\n",
    "**RESULT**\n",
    "\n",
    "| subject_id |    time     | mesurement | dose |\n",
    "|:----------:|:----------:|:----------:|:----:|\n",
    "|    01      | 1751875000 |    2.5     |      |\n",
    "|    01      | 1751954400 |    2.9     |  5   |\n",
    "|    01      | 1752226200 |    2.7     | 15   |\n",
    "|    02      | 1751880000 |    2.0     | 15   |\n",
    "|    02      | 1752092600 |    2.0     | 12   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
