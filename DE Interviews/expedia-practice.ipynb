{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a847bb03",
   "metadata": {},
   "source": [
    "### Spark architecture\n",
    "- Spark follows a master-slave architecture with a central Driver and multiple Executors.\n",
    "- The Driver manages the SparkContext, coordinates tasks, and maintains metadata.\n",
    "- Executors run on worker nodes, execute tasks, and store data in memory or disk.\n",
    "- Cluster managers (like YARN, Mesos, or Kubernetes) allocate resources for Spark applications.\n",
    "- Spark supports multiple deployment modes: local, standalone, and on external cluster managers.\n",
    "\n",
    "### Spark optimizations and scenario based\n",
    "- Use predicate pushdown and column pruning to minimize data read.\n",
    "- Cache/persist intermediate DataFrames when reused multiple times.\n",
    "- Leverage broadcast joins for small lookup tables to avoid shuffles.\n",
    "- Repartition or coalesce DataFrames to optimize parallelism and avoid data skew.\n",
    "- Tune Spark configurations (memory, shuffle partitions, executor cores) based on workload.\n",
    "\n",
    "### Optimizing your Spark SQL queries\n",
    "\n",
    "1. **Partition Pruning:** Use partitioned tables and filter on partition columns to minimize data scanned.\n",
    "2. **Predicate Pushdown:** Ensure filters are pushed down to the data source (like Parquet/ORC) to reduce data read.\n",
    "3. **Column Pruning:** Select only required columns to reduce I/O and memory usage.\n",
    "4. **Broadcast Joins:** Use broadcast joins for small tables to avoid expensive shuffles.\n",
    "5. **Avoid UDFs When Possible:** Prefer built-in Spark SQL functions over UDFs for better performance and optimization.\n",
    "\n",
    "### Broadcasting\n",
    "- Broadcasting sends a small DataFrame to all worker nodes, enabling efficient joins by avoiding shuffles.\n",
    "- Use `broadcast()` in Spark when joining a large table with a much smaller one to optimize performance.\n",
    "\n",
    "### Long running jobs, how you will debug?\n",
    "- Check Spark UI for slow stages, skewed tasks, or failed jobs.  \n",
    "\n",
    "- Review job DAG and execution plan for bottlenecks.  \n",
    "\n",
    "- Inspect stage/task metrics for data skew, long task durations, or high shuffle read/write.  \n",
    "\n",
    "- Monitor executor and driver logs for errors, GC pauses, or resource issues.  \n",
    "\n",
    "- Validate cluster resource allocation (CPU, memory, disk, network).  \n",
    "\n",
    "- Ensure efficient data partitioning and avoid small files or partitions.  \n",
    "\n",
    "- Check for excessive shuffles, wide transformations, or large broadcast joins. \n",
    "\n",
    "- Confirm that Spark configurations (memory, cores, partitions) are tuned for the workload.\n",
    "\n",
    "### How to check UI for slow stages, skewed tasks, or failed jobs.  \n",
    "\n",
    "- Open the Spark UI (usually accessible at `http://<driver-node>:4040` during job execution).\n",
    "- Navigate to the **Stages** tab to identify stages with high duration, many failed tasks, or significant skew (uneven task durations).\n",
    "- Use the **Jobs** tab to see job status and failure reasons.\n",
    "- Drill down into individual stages and tasks to inspect metrics like task time, input size, and shuffle read/write.\n",
    "- Check the **Executors** tab for resource usage, task failures, and GC overhead.\n",
    "- Review logs for error messages or stack traces related to failed jobs or tasks.\n",
    "\n",
    "##### If pipelines are running from so long, how you will debug it? what can be potential bottleneck/issues?\n",
    "\n",
    "- Analyze Spark UI for slow or stuck stages, long-running tasks, and task failures.\n",
    "- Check for data skew (some partitions processing much more data than others).\n",
    "- Review shuffle operationsâ€”excessive shuffling can cause performance bottlenecks.\n",
    "- Inspect resource utilization (CPU, memory, disk, network) on executors and driver.\n",
    "- Look for frequent garbage collection or out-of-memory errors in logs.\n",
    "- Validate input data size and partitioning; too many or too few partitions can impact performance.\n",
    "- Check for inefficient joins (e.g., missing broadcast joins, large shuffles).\n",
    "- Review code for expensive operations (UDFs, wide transformations, unnecessary caching).\n",
    "- Ensure cluster resources are sufficient and not over-allocated to other jobs.\n",
    "- Investigate external dependencies (slow data sources, network latency, etc.).\n",
    "- Use event logs and metrics to trace where the pipeline is spending most of its time.\n",
    "\n",
    "###### Try catch and alert mechanisms implemented in your project , for what purpose do you use them?\n",
    "- In our project, try-catch blocks are used to gracefully handle data ingestion errors, such as malformed records or connectivity issues, ensuring the pipeline continues processing valid data while logging or alerting on failures.\n",
    "- Alert mechanisms (like email, Slack, or monitoring tools) notify the team of job failures, data quality issues, or resource bottlenecks, enabling quick response and minimizing downtime.\n",
    "\n",
    "Different file types\n",
    "| File Type   | Schema Evolution | Compression Support | Read Speed     | Write Speed    | ACID Support | Partitioning | Iceberg Integration |\n",
    "|-------------|------------------|--------------------|----------------|----------------|--------------|--------------|---------------------|\n",
    "| CSV         | No               | No (external only) | Slow           | Fast           | No           | Manual       | No                  |\n",
    "| JSON        | No               | No (external only) | Slow           | Fast           | No           | Manual       | No                  |\n",
    "| Parquet     | Limited (add cols)| Yes                | Fast           | Fast           | No           | Yes          | Yes                 |\n",
    "| ORC         | Limited (add cols)| Yes                | Fast           | Fast           | No           | Yes          | Yes                 |\n",
    "| Delta Lake  | Yes              | Yes                | Fast           | Fast           | Yes          | Yes          | Yes                 |\n",
    "| Iceberg     | Yes (full)       | Yes                | Fast           | Fast           | Yes          | Yes          | Native              |\n",
    "\n",
    "\n",
    "##### Delta table/Deltalake concepts\n",
    "\n",
    "- Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.\n",
    "- Supports schema enforcement and evolution, allowing safe changes to table structure over time.\n",
    "- Enables time travel, letting users query previous versions of data using version numbers or timestamps.\n",
    "- Provides scalable metadata handling, making it efficient for large tables with many partitions.\n",
    "- Supports upserts, deletes, and merges using the `MERGE INTO` command for complex ETL operations.\n",
    "\n",
    "#### Why does Driver go OOM (Out Of Memory), solutions and preventions\n",
    "\n",
    "- **Causes:**\n",
    "    - Collecting large datasets to the driver using actions like `.collect()`, `.toPandas()`, or `.take()` on big DataFrames/RDDs.\n",
    "    - Accumulating large broadcast variables or driver-side data structures.\n",
    "    - Excessive logging or storing large objects in driver memory.\n",
    "    - High concurrency or too many tasks scheduled at once.\n",
    "\n",
    "- **Solutions & Preventions:**\n",
    "    - Avoid collecting large datasets to the driver; use distributed operations and aggregate results before collecting.\n",
    "    - Increase driver memory via `--driver-memory` or `spark.driver.memory` configuration.\n",
    "    - Use `persist()` or `cache()` judiciously to avoid unnecessary memory usage.\n",
    "    - Limit the size of broadcast variables and driver-side data structures.\n",
    "    - Optimize Spark code to minimize driver-side data accumulation.\n",
    "    - Monitor driver memory usage and tune Spark configurations accordingly.\n",
    "    - Use logging levels appropriately to avoid excessive log data in memory.\n",
    "\n",
    "#### Why does Executor go OOM (Out Of Memory), solutions and preventions\n",
    "\n",
    "- **Causes:**\n",
    "    - Processing partitions that are too large, causing a single executor to exceed its memory limits.\n",
    "    - Inefficient joins (e.g., shuffles, skewed data, or large broadcast joins).\n",
    "    - Caching/persisting large DataFrames/RDDs without enough memory.\n",
    "    - Memory leaks or holding references to large objects in user code.\n",
    "    - Too many concurrent tasks per executor, leading to memory contention.\n",
    "    - Uncompressed or poorly compressed data formats increasing memory usage.\n",
    "\n",
    "- **Solutions & Preventions:**\n",
    "    - Repartition data to ensure partitions are of manageable size.\n",
    "    - Use `persist()`/`cache()` judiciously and unpersist data when no longer needed.\n",
    "    - Tune executor memory (`spark.executor.memory`) and core settings for workload.\n",
    "    - Avoid wide transformations on skewed data; use salting or custom partitioning to balance load.\n",
    "    - Prefer built-in Spark functions over UDFs to reduce memory overhead.\n",
    "    - Monitor executor memory usage and adjust Spark configurations as needed.\n",
    "    - Use efficient data formats (like Parquet/ORC) and enable compression.\n",
    "    - Limit the number of concurrent tasks per executor (`spark.executor.cores`).\n",
    "    - Regularly review and optimize Spark code to avoid unnecessary memory retention.\n",
    "\n",
    "#### Catalyst Optimizer and SQL Optimization Plans\n",
    "\n",
    "- The Catalyst Optimizer is Spark SQLâ€™s query optimization framework, responsible for transforming SQL queries into efficient execution plans.\n",
    "- It uses a tree-based architecture to represent and optimize queries through a series of rule-based and cost-based transformations.\n",
    "\n",
    "**Different Plans in Catalyst Optimizer:**\n",
    "\n",
    "| Plan Type                | Description                                                      |\n",
    "|--------------------------|------------------------------------------------------------------|\n",
    "| Unresolved Logical Plan  | Parsed query, unresolved references                              |\n",
    "| Analyzed Logical Plan    | References resolved, validated against schema/catalog            |\n",
    "| Optimized Logical Plan   | Logical optimizations applied                                    |\n",
    "| Physical Plan(s)         | Concrete execution strategies generated                          |\n",
    "| Executed Plan            | Final plan with RDD operations, ready for execution              |\n",
    "\n",
    "- Catalyst enables extensibility for custom rules, supports advanced optimizations, and is key to Spark SQLâ€™s performance.\n",
    "\n",
    "##### Spark Jobs,stages,tasks and how it is decided how many jobs/stages/tasks will be created of a spark application?\n",
    "\n",
    "- **Jobs:** A Spark job is triggered by an action (like `collect()`, `save()`, `count()`) on a DataFrame/RDD. Each action creates a new job.\n",
    "- **Stages:** Each job is divided into stages based on shuffle boundaries. A stage is a set of tasks that can be executed in parallel, separated by wide transformations (like `groupBy`, `join`).\n",
    "- **Tasks:** A stage is split into tasks, one per data partition. Each task processes a single partition of data.\n",
    "\n",
    "**How are the numbers decided?**\n",
    "- The number of **jobs** equals the number of actions invoked.\n",
    "- The number of **stages** is determined by the number of shuffle boundaries in the jobâ€™s DAG. Each wide transformation introduces a new stage.\n",
    "- The number of **tasks** in each stage equals the number of partitions in the input RDD/DataFrame for that stage. Partitioning can be controlled via `repartition()`, `coalesce()`, or data source partitioning.\n",
    "\n",
    "**Summary:**  \n",
    "- Actions â†’ Jobs  \n",
    "- Shuffle boundaries â†’ Stages  \n",
    "- Partitions â†’ Tasks\n",
    "\n",
    "##### Spark shuffling (sort-merge/hash) \n",
    "- **Shuffling** is the process of redistributing data across partitions, typically required by wide transformations like `groupBy`, `join`, or `distinct`. It is an expensive operation involving disk and network I/O.\n",
    "\n",
    "- **Sort-Merge Shuffle:** Used for operations like `sortBy` or `sort-merge join`. Data is sorted within each partition and then merged. It is efficient for large datasets but can be slower due to sorting overhead.\n",
    "\n",
    "- **Hash Shuffle:** Used for hash-based operations like `groupByKey` or `reduceByKey`. Data is assigned to partitions based on a hash function. It is generally faster for smaller datasets but can cause memory pressure and data skew if keys are unevenly distributed.\n",
    "\n",
    "- **Optimizations:**\n",
    "    - Use `reduceByKey` instead of `groupByKey` to minimize data shuffled.\n",
    "    - Tune `spark.sql.shuffle.partitions` to control the number of shuffle partitions.\n",
    "    - Prefer broadcast joins for small tables to avoid shuffling large datasets.\n",
    "    - Monitor shuffle read/write metrics in Spark UI to identify bottlenecks.\n",
    "\n",
    "##### Data skewness - how you will handle it? \n",
    "- **Salting:** Add a random prefix or suffix to skewed keys to distribute data more evenly across partitions, then remove the salt after aggregation.\n",
    "- **Custom Partitioning:** Implement a custom partitioner to control how data is distributed, ensuring skewed keys are spread out.\n",
    "- **Broadcast Join:** Use broadcast joins when one side of the join is small to avoid shuffling large, skewed datasets.\n",
    "- **Skew Join Hints:** Use Spark SQLâ€™s `skew` join hints to handle skewed joins automatically.\n",
    "- **Splitting Skewed Keys:** Process skewed keys separately from the rest of the data, then union the results.\n",
    "- **Increase Shuffle Partitions:** Increase the number of shuffle partitions to reduce the amount of data per partition.\n",
    "- **Filter Outliers:** If possible, filter or pre-aggregate highly skewed keys before the main computation.\n",
    "\n",
    "Spark UI - uses, how it helps in debugging\n",
    "\n",
    "DAG,lazy evaluation,narrow/wide transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407abc4c",
   "metadata": {},
   "source": [
    "SQL: \n",
    "user_id,website,time\n",
    "user1,Site1,10:00\n",
    "user1,Site2,10:17\n",
    "user1,Site3,10:35\n",
    "user1,Site4,11:00\n",
    "user2,Site1,11:15\n",
    "user2,Site2,11:32\n",
    "user2,Site3,12:05\n",
    "user2,Site4,12:20\n",
    "user3,Site1,12:30\n",
    "user3,Site2,13:05\n",
    "user3,Site3,13:15\n",
    "user3,Site4,13:37\n",
    "  \n",
    "Find the time for each User,for each site they have spend on? (considering 1st site default to 0 for each user)\n",
    "Example - User1 at site2 , spent 10:00-10:17 = 17mins,\n",
    "                 user1 at site3, spent 10:35-10:17 = 18mins and so on....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e6d159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+----------------+\n",
      "|user_id|website| time|duration_in_mins|\n",
      "+-------+-------+-----+----------------+\n",
      "|  user1|  Site1|10:00|             0.0|\n",
      "|  user1|  Site2|10:17|            17.0|\n",
      "|  user1|  Site3|10:35|            18.0|\n",
      "|  user1|  Site4|11:00|            25.0|\n",
      "|  user2|  Site1|11:15|             0.0|\n",
      "|  user2|  Site2|11:32|            17.0|\n",
      "|  user2|  Site3|12:05|            33.0|\n",
      "|  user2|  Site4|12:20|            15.0|\n",
      "|  user3|  Site1|12:30|             0.0|\n",
      "|  user3|  Site2|13:05|            35.0|\n",
      "|  user3|  Site3|13:15|            10.0|\n",
      "|  user3|  Site4|13:37|            22.0|\n",
      "+-------+-------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Manna\").getOrCreate()\n",
    "columns = [\"user_id\",\"website\",\"time\"]\n",
    "data = [\n",
    "    (\"user1\", \"Site1\", \"10:00\"),\n",
    "    (\"user1\", \"Site2\", \"10:17\"),\n",
    "    (\"user1\", \"Site3\", \"10:35\"),\n",
    "    (\"user1\", \"Site4\", \"11:00\"),\n",
    "    (\"user2\", \"Site1\", \"11:15\"),\n",
    "    (\"user2\", \"Site2\", \"11:32\"),\n",
    "    (\"user2\", \"Site3\", \"12:05\"),\n",
    "    (\"user2\", \"Site4\", \"12:20\"),\n",
    "    (\"user3\", \"Site1\", \"12:30\"),\n",
    "    (\"user3\", \"Site2\", \"13:05\"),\n",
    "    (\"user3\", \"Site3\", \"13:15\"),\n",
    "    (\"user3\", \"Site4\", \"13:37\"),\n",
    "]\n",
    "spark.sparkContext.setJobDescription(\"Loading data from CSV file\")\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.createOrReplaceTempView(\"users\")\n",
    "spark.sparkContext.setJobDescription(\"Applying Transformation\")\n",
    "res=spark.sql(\"\"\"\n",
    "    select *,\n",
    "        coalesce(cast(to_timestamp(time,'HH:mm')-to_timestamp(lag(time) over(partition by user_id order by time),'HH:mm') as int )/60,0) as duration_in_mins\n",
    "        from users\n",
    "\"\"\")\n",
    "spark.sparkContext.setJobDescription(\"Showing Data\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80654466",
   "metadata": {},
   "source": [
    "You have a csv file, and there are some **malformed records** in it...instead of stopping and failing the ingestion, you need to implement a try-catch block to handle the malformed records, so correct records will be processed further and malformed will be filtered out and stored in error path.\n",
    "\n",
    "To handle malformed CSV records in Spark, use the `mode` and `badRecordsPath` options in `spark.read.csv`. Example:\n",
    "- Use `mode(\"PERMISSIVE\")` (default) to set malformed fields to `null`.\n",
    "- Use `mode(\"DROPMALFORMED\")` to drop malformed rows.\n",
    "- Use `mode(\"FAILFAST\")` to fail on malformed records.\n",
    "- To capture bad records, use `option(\"badRecordsPath\", \"/path/to/error_dir\")`.\n",
    "Example code:\n",
    "```python\n",
    "df = spark.read.option(\"header\", True) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"badRecordsPath\", \"/path/to/error_dir\") \\\n",
    "    .csv(\"/path/to/input.csv\")\n",
    "```\n",
    "This will process valid records and store malformed ones in the specified error path for further inspection.\n",
    "\n",
    "####  Decorators in python\n",
    "- flask for check session or role\n",
    "- for logging\n",
    "\n",
    "You can define a simple logging decorator and use it to log function calls:\n",
    "\n",
    "```python\n",
    "import functools\n",
    "\n",
    "def log_function_call(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(f\"Calling function: {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"Function {func.__name__} finished\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@log_function_call\n",
    "def process_data(df):\n",
    "    return df.count()\n",
    "\n",
    "# Usage\n",
    "process_data(df)\n",
    "```\n",
    "\n",
    "This will print log messages before and after the `process_data` function is called.\n",
    "\n",
    "#### Dataset vs Dataframe\n",
    "\n",
    "- **Dataset**: Strongly-typed, distributed collection of data. Available in Scala and Java (not in Python). Supports compile-time type safety and functional transformations.\n",
    "- **DataFrame**: Distributed collection of data organized into named columns (like a table). Available in Python, Scala, Java, and R. Offers high-level APIs, optimized by Catalyst, and is untyped in Python (no compile-time type safety).\n",
    "- In PySpark, you work with DataFrames (no Dataset API). DataFrames are the primary abstraction for structured data processing in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3b8c17",
   "metadata": {},
   "source": [
    "\n",
    "22. Write pyspark code : \n",
    "A) Input:  1|aaa|111|2|bbb|222|3|ccc|333\n",
    "output: \n",
    "1 aaa 111\n",
    "2 bbb 222\n",
    "3 ccc 333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110c0577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---+---+---+\n",
      "|               value|splitted_val|rnm| l1| l2|\n",
      "+--------------------+------------+---+---+---+\n",
      "|1|aaa|111|2|bbb|2...|           1|  1|aaa|111|\n",
      "|1|aaa|111|2|bbb|2...|           2|  4|bbb|222|\n",
      "|1|aaa|111|2|bbb|2...|           3|  7|ccc|333|\n",
      "+--------------------+------------+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/05 12:50:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Manna\").getOrCreate()\n",
    "input_text = \"1|aaa|111|2|bbb|222|3|ccc|333\"\n",
    "df = spark.createDataFrame([(input_text,)], [\"value\"])\n",
    "df2 = df\\\n",
    "        .withColumn(\"splitted_val\", F.explode(F.split(\"value\", \"\\|\")))\\\n",
    "        .withColumn(\"rnm\", F.row_number().over(Window.orderBy(\"value\")))\\\n",
    "        .withColumn(\"l1\", F.lead(\"splitted_val\").over(Window.orderBy(\"rnm\")))\\\n",
    "        .withColumn(\"l2\", F.lead(\"splitted_val\", 2).over(Window.orderBy(\"rnm\")))\\\n",
    "        .filter((F.col(\"rnm\") - 1) % 3 == 0)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa55e3",
   "metadata": {},
   "source": [
    "\n",
    "B)  Input:\n",
    "|Product|Amount|Country|\n",
    "+-------+------+-------+\n",
    "| Banana|  1000|    USA|\n",
    "|Carrots|  1500|    USA|\n",
    "|  Beans|  1600|   USA|\n",
    "| Orange|  2000|    USA|\n",
    "| Orange|  2000|    USA|\n",
    "| Banana|   400|  China|\n",
    "|Carrots|  1200|  China|\n",
    "|  Beans|  1500| China|\n",
    "| Orange|  4000|  China|\n",
    "| Banana|  2000| Canada|\n",
    "|Carrots|  2000| Canada|\n",
    "|  Beans|  2000| Mexico|\n",
    "+-------+-----+-------+\n",
    "Output\n",
    "+-------+------+-----+------+----+\n",
    "|Product|Canada|China|Mexico| USA|\n",
    "+-------+------+-----+------+----+\n",
    "| Orange|  null| 4000|  null|4000|\n",
    "|  Beans|  null| 1500| 2000|1600|\n",
    "| Banana|  2000|  400| null|1000|\n",
    "|Carrots|  2000| 1200|  null|1500| \n",
    "\n",
    "```\n",
    " select product,\n",
    "        sum( case when(country = 'Canada') then amount else null end ) as Canada,\n",
    "        sum( case when(country = 'China') then amount else null end ) as China,\n",
    "        sum( case when(country = 'Mexico') then amount else null end ) as Mexico,\n",
    "        sum( case when(country = 'USA') then amount else null end ) as USA\n",
    "        from products\n",
    "        group by product\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
