import findspark
findspark.init()
import os
import pytest
import pyspark
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.window import Window
import glob
from functools import reduce
from pyspark.sql import DataFrame
import re

def get_spark():
    return SparkSession.builder.master("local[*]").appName("Pytest-PySpark").getOrCreate()

def read_csv(spark, file_path):
    return spark.read.csv(file_path, header=True, inferSchema=True)

def transform_with_sql(spark, dfs):
    current_file = os.path.splitext(os.path.basename(__file__))[0]
    input_files = sorted(glob.glob(f"SQL/hard/{current_file}/input-*.csv"))
    for file, df in zip(input_files, dfs):
        match = re.search(r'input-(.*)\.csv', os.path.basename(file))
        if match:
            view_name = match.group(1)
            df.createOrReplaceTempView(view_name)

    result_df = spark.sql("""
        with prev_malware as (
            select software_id, malware_detected as latest_run_count,
            (malware_detected - (lag(malware_detected) over(partition by software_id order by run_date))) as difference_to_previous,
            count(software_id) over(partition by software_id) as total_count,
            row_number() over(partition by software_id order by run_date desc) as row_num
            from malware
        )
        select  software_id, latest_run_count, difference_to_previous
        from prev_malware 
        where total_count >=2 and latest_run_count >= 10 and row_num = 1
    """)
    result_df.show()
    return result_df

def transform_with_pyspark(dfs):
    malware_df = dfs[0]
    res_df = malware_df\
        .withColumn('difference_to_previous', (F.col('malware_detected')-F.lead(F.col('malware_detected')).over(Window.partitionBy('software_id').orderBy(F.col('run_date').desc()))))\
        .withColumn('total_count',F.count('software_id').over(Window.partitionBy('software_id')))\
        .withColumn('row_num',F.row_number().over(Window.partitionBy('software_id').orderBy(F.col('run_date').desc())))\
        .filter((F.col('total_count') >= 2) & (F.col('malware_detected') >= 10) & (F.col('row_num') == 1))\
        .select(F.col('software_id'),F.col('malware_detected').alias('latest_run_count'),F.col('difference_to_previous'))
    res_df.show()
    return res_df


def compare_dataframes(expected_df, actual_df):
    expected_cols = set(expected_df.columns)
    actual_cols = set(actual_df.columns)
    if expected_cols != actual_cols:
        print(f"Column mismatch: expected {expected_cols}, actual {actual_cols}")
        return False

    sort_columns = list(expected_df.columns)
    expected_sorted = expected_df.orderBy(*sort_columns)
    actual_sorted = actual_df.orderBy(*sort_columns)

    expected_rows = expected_sorted.collect()
    actual_rows = actual_sorted.collect()

    if len(expected_rows) != len(actual_rows):
        print(f"Row count mismatch: expected {len(expected_rows)}, actual {len(actual_rows)}")
        return False

    for idx, (exp_row, act_row) in enumerate(zip(expected_rows, actual_rows)):
        for col in sort_columns:
            if exp_row[col] != act_row[col]:
                print(f"Mismatch at row {idx}, column '{col}': expected {exp_row[col]}, actual {act_row[col]}")
                return False
    return True

def test_transformations(spark):
    current_file = os.path.splitext(os.path.basename(__file__))[0]
    input_files = sorted(glob.glob(f"SQL/hard/{current_file}/input-*.csv"))
    output_file = f"SQL/hard/{current_file}/output.csv"

    input_dfs = [read_csv(spark, file) for file in input_files]
    expected_output_df = read_csv(spark, output_file)

    sql_result_df = transform_with_sql(spark, input_dfs)
    pyspark_result_df = transform_with_pyspark(input_dfs)
    try:
        sort_columns = expected_output_df.columns
        expected_output_df = expected_output_df.orderBy(*sort_columns)
        sql_result_df = sql_result_df.orderBy(*sort_columns)
        pyspark_result_df = pyspark_result_df.orderBy(*sort_columns)
    except Exception as e:
        print(e)
        pass

    # Assert that the SQL transformation matches the expected output
    assert compare_dataframes(expected_output_df,sql_result_df)

    # Assert that the PySpark transformation matches the expected output
    assert compare_dataframes(expected_output_df,pyspark_result_df)

if __name__=="__main__":
    test_transformations(get_spark())